{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import desc\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = \"twitter_spark\"\n",
    "user = \"root\"\n",
    "password  = \"Qwertyw@123\"\n",
    "path = \"/home/nineleaps/PycharmProjects/spark_twitter/data/\"\n",
    "\n",
    "# path = \"/home/nineleaps/Desktop/Day-22-24-20200610T055939Z-001/Day-22-24/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engine(mysql+pymysql://root:***@localhost/twitter_spark)\n"
     ]
    }
   ],
   "source": [
    "engine = create_engine(\"mysql+pymysql://{user}:{pw}@localhost/{db}\"\n",
    "                       .format(user=\"root\",\n",
    "                               pw=\"Qwerty@123\",\n",
    "                               db=\"twitter_spark\"))\n",
    "print(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark\\\n",
    ".readStream.format(\"socket\")\\\n",
    ".option(\"host\",\"localhost\")\\\n",
    ".option(\"port\",\"9001\")\\\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f5e65604da0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\\\n",
    ".writeStream\\\n",
    ".format(\"json\")\\\n",
    ".option(\"path\", path)\\\n",
    ".option(\"checkpointLocation\", path)\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nineleaps/Desktop/Day-22-24-20200610T055939Z-001/Day-22-24/data/part-00000-c67d1a3b-45a9-4928-b966-970d63718f2f-c000.json\n"
     ]
    }
   ],
   "source": [
    "# Getting JSON File Name that Contains Data.\n",
    "\n",
    "file_path=[]\n",
    "main_file = ''\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith(\".json\"):\n",
    "        file_path.append(path+file)\n",
    "\n",
    "for file in file_path:\n",
    "    if os.stat(file).st_size != 0:\n",
    "        main_file = file\n",
    "        \n",
    "print(main_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Data From JSON File and Storing Into Dataframe.\n",
    "\n",
    "f = open(main_file, \"r\")\n",
    "file_content = json.loads(f.read())['value']\n",
    "\n",
    "s = list(file_content.split('}'))\n",
    "l = s\n",
    "arr=[]\n",
    "s[:] = [i + '}' for i in s]\n",
    "for i in l[:-1]:\n",
    "    arr.append(json.loads(i))\n",
    "\n",
    "for t in arr:\n",
    "    temp = t['hashtag'].split(',')\n",
    "    t['hashtag'] = ','.join(list(set(temp)))\n",
    "\n",
    "df = pd.DataFrame(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populating Data Into SQL Table.\n",
    "\n",
    "df.to_sql('feed_data', con = engine, if_exists = 'replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Ghana', '#Hushpuppi', 'Wed Jun 10 09:51:31 +0000 2020')\n",
      "('United States', '#GoneWithTheWind', 'Wed Jun 10 09:51:32 +0000 2020')\n",
      "('India', '#1YearOfFainatsTBK', 'Wed Jun 10 09:51:34 +0000 2020')\n"
     ]
    }
   ],
   "source": [
    "# Reading Data From SQL Table.\n",
    "\n",
    "result = engine.execute('SELECT* FROM feed_data')\n",
    "for i in result.fetchmany(3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+--------------------+\n",
      "|      country|             hashtag|                time|\n",
      "+-------------+--------------------+--------------------+\n",
      "|        Ghana|          #Hushpuppi|Wed Jun 10 09:51:...|\n",
      "|United States|    #GoneWithTheWind|Wed Jun 10 09:51:...|\n",
      "|        India|  #1YearOfFainatsTBK|Wed Jun 10 09:51:...|\n",
      "|United States|          #Pakistani|Wed Jun 10 09:51:...|\n",
      "|United States|  #earthquake,#quake|Wed Jun 10 09:51:...|\n",
      "|United States|  #earthquake,#quake|Wed Jun 10 09:51:...|\n",
      "|        Kenya|  #CoronaHasTaughtMe|Wed Jun 10 09:51:...|\n",
      "|        India|#CoronaFreeBharat...|Wed Jun 10 09:51:...|\n",
      "|        Ghana|       #DaybreakHitz|Wed Jun 10 09:51:...|\n",
      "|        India|#JhootiHaiBiharSa...|Wed Jun 10 09:51:...|\n",
      "+-------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating Spark Dataframe Object.\n",
    "\n",
    "myJson = sc.parallelize(arr)\n",
    "myDf = sqlContext.read.json(myJson)\n",
    "myDf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             country|most_popular_hashtag|\n",
      "+--------------------+--------------------+\n",
      "|Hashemite Kingdom...|         #NiajeNiaje|\n",
      "|             T端rkiye|               #MOOD|\n",
      "|               Ghana|       #DaybreakHitz|\n",
      "|       United States|         #earthquake|\n",
      "|               India|  #1YearOfFainatsTBK|\n",
      "|             Nigeria|#LateMorningMusic...|\n",
      "|             Grenada|          #Ghosttown|\n",
      "|              Uganda|        #VisitUganda|\n",
      "|               Kenya|  #CoronaHasTaughtMe|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Getting Location wise most popular hashtags.\n",
    "\n",
    "my_li = []\n",
    "unique_country_list = myDf.select('country').distinct().collect()\n",
    "\n",
    "for country in unique_country_list:\n",
    "    a = myDf.filter(myDf.country == country[0]).select(myDf.hashtag).collect()\n",
    "    li=[]\n",
    "    for i in a:\n",
    "        li.extend([j.lstrip() for j in i[0].split(',')])\n",
    "    \n",
    "    d = defaultdict(int)\n",
    "    for i in li:\n",
    "        d[i] += 1\n",
    "    result = max(d.items(), key=lambda x: x[1])\n",
    "\n",
    "        \n",
    "    my_li.append({\n",
    "        'country': country[0],\n",
    "        'most_popular_hashtag': result[0]\n",
    "    })\n",
    "\n",
    "# Creating Spark Dataframe Object.\n",
    "myJson1 = sc.parallelize(my_li)\n",
    "df1 = sqlContext.read.json(myJson1)\n",
    "df1.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populating Data Into SQL Table.\n",
    "\n",
    "df1_sql = pd.DataFrame(my_li)\n",
    "df1_sql.to_sql('most_popular_hashtag', con = engine, if_exists = 'replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Hashemite Kingdom of Jordan', '#NiajeNiaje')\n",
      "('T端rkiye', '#MOOD')\n",
      "('Ghana', '#DaybreakHitz')\n"
     ]
    }
   ],
   "source": [
    "# Reading Data From SQL Table.\n",
    "\n",
    "result = engine.execute('SELECT* FROM most_popular_hashtag')\n",
    "for i in result.fetchmany(3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+\n",
      "|combination_hashtags|      country|most_popular_hashtag|\n",
      "+--------------------+-------------+--------------------+\n",
      "|              #quake|United States|         #earthquake|\n",
      "|       #summerwalker|      Nigeria|#LateMorningMusic...|\n",
      "|       #LakeBunyonyi|       Uganda|        #VisitUganda|\n",
      "+--------------------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Getting most popular hashtags used in combination with.\n",
    "\n",
    "comb_list = []\n",
    "for i in my_li:\n",
    "    a = myDf.filter(myDf[\"hashtag\"].contains(i['most_popular_hashtag'])).select(myDf['hashtag']).collect()\n",
    "    \n",
    "    hashtags_list=[]\n",
    "    for k in a:\n",
    "        hashtags_list.extend([j.lstrip() for j in k[0].split(',')])\n",
    "        \n",
    "    li = [j for j in hashtags_list if j != i['most_popular_hashtag']]\n",
    "    comb_hashtag = ','.join(list(set(li)))\n",
    "\n",
    "    if comb_hashtag != '':        \n",
    "        comb_list.append({\n",
    "        'country': i['country'],\n",
    "        'most_popular_hashtag': i['most_popular_hashtag'],\n",
    "        'combination_hashtags': comb_hashtag})\n",
    "        \n",
    "# Creating Spark Dataframe Object.\n",
    "myJson2 = sc.parallelize(comb_list)\n",
    "df2 = sqlContext.read.json(myJson2)\n",
    "df2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populating Data Into SQL Table.\n",
    "\n",
    "df2_sql = pd.DataFrame(comb_list)\n",
    "df2_sql.to_sql('most_popular_hashtag_combination', con = engine, if_exists = 'replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('United States', '#earthquake', '#quake')\n",
      "('Nigeria', '#LateMorningMusicMixLive', '#summerwalker')\n",
      "('Uganda', '#VisitUganda', '#LakeBunyonyi')\n"
     ]
    }
   ],
   "source": [
    "# Reading Data From SQL Table.\n",
    "\n",
    "result = engine.execute('SELECT* FROM most_popular_hashtag_combination')\n",
    "for i in result.fetchmany(3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+\n",
      "|             country|             hashtag|count|\n",
      "+--------------------+--------------------+-----+\n",
      "|               Ghana|       #DaybreakHitz|    2|\n",
      "|               Kenya|  #CoronaHasTaughtMe|    1|\n",
      "|              Uganda|        #VisitUganda|    1|\n",
      "|       United States|         #earthquake|    2|\n",
      "|             Grenada|          #Ghosttown|    1|\n",
      "|             T端rkiye|               #MOOD|    1|\n",
      "|             Nigeria|#LateMorningMusic...|    1|\n",
      "|Hashemite Kingdom...|         #NiajeNiaje|    1|\n",
      "|               India|  #1YearOfFainatsTBK|    2|\n",
      "+--------------------+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Getting Tweet Frequency.\n",
    "\n",
    "list3 = []\n",
    "\n",
    "for i in my_li:\n",
    "    a = myDf.filter(myDf[\"hashtag\"].contains(i['most_popular_hashtag'])).select([myDf['hashtag'], myDf['time']]).collect()\n",
    "    for j in a:\n",
    "          list3.append({\n",
    "              'country': i['country'],\n",
    "              'hashtag': i['most_popular_hashtag'],\n",
    "              'time': j[1]\n",
    "          })\n",
    "            \n",
    "# Creating Spark Dataframe Object.\n",
    "myJson3 = sc.parallelize(list3)\n",
    "df3 = sqlContext.read.json(myJson3)\n",
    "df3.groupby(\"country\",\"hashtag\").count().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populating Data Into SQL Table.\n",
    "\n",
    "df3_sql = pd.DataFrame(list3)\n",
    "df3_sql.to_sql('tweet_frequency', con = engine, if_exists = 'replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Hashemite Kingdom of Jordan', '#NiajeNiaje', 'Wed Jun 10 09:52:15 +0000 2020')\n",
      "('T端rkiye', '#MOOD', 'Wed Jun 10 09:51:55 +0000 2020')\n",
      "('Ghana', '#DaybreakHitz', 'Wed Jun 10 09:51:46 +0000 2020')\n"
     ]
    }
   ],
   "source": [
    "# Reading Data From SQL Table.\n",
    "\n",
    "result = engine.execute('SELECT* FROM tweet_frequency')\n",
    "for i in result.fetchmany(3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
